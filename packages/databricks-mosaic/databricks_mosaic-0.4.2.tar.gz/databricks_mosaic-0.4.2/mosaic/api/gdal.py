from .fuse import SetupMgr
from pyspark.sql import SparkSession

import subprocess

__all__ = ["setup_gdal", "enable_gdal"]


def setup_gdal(
        to_fuse_dir: str = "/Workspace/Shared/geospatial/mosaic/gdal/jammy/0.4.2",
        script_out_name: str = "mosaic-gdal-init.sh",
        jni_so_copy: bool = False,
        test_mode: bool = False
) -> bool:
    """
    Prepare GDAL init script and shared objects required for GDAL to run on spark.
    This function will generate the init script that will install GDAL on each worker node.
    After the setup_gdal is run, the init script must be added to the cluster; also,
    a cluster restart is required.

    Notes:
      (a) This is close in behavior to Mosaic < 0.4 series (prior to DBR 13),
          now using jammy default (3.4.1)
      (b) `to_fuse_dir` can be one of `/Volumes/..`, `/Workspace/..`, `/dbfs/..`;
           however, you should use `setup_fuse_install()` for Volume based installs

    Parameters
    ----------
    to_fuse_dir : str
            Path to write out the init script for GDAL installation;
            default is '/Workspace/Shared/geospatial/mosaic/gdal/jammy/0.4.2'.
    script_out_name : str
            name of the script to be written;
            default is 'mosaic-gdal-init.sh'.
    jni_so_copy : bool
            if True, copy shared object to fuse dir and config script to use;
            default is False
    test_mode : bool
            Only for unit tests.

    Returns
    -------
    True unless resources fail to download.
    """
    setup_mgr = SetupMgr(
        to_fuse_dir,
        script_out_name=script_out_name,
        jni_so_copy=jni_so_copy
    )
    return setup_mgr.configure(test_mode=test_mode)


def enable_gdal(spark: SparkSession) -> None:
    """
    Enable GDAL at runtime on a cluster with GDAL installed using init script,
    e.g.  generated by setup_gdal() or setup_fuse_install() call.

    Parameters
    ----------
    spark : pyspark.sql.SparkSession
            The active SparkSession.

    Returns
    -------

    """
    try:
        sc = spark.sparkContext
        mosaicGDALObject = getattr(
            sc._jvm.com.databricks.labs.mosaic.gdal, "MosaicGDAL"
        )
        mosaicGDALObject.enableGDAL(spark._jsparkSession)
        print("GDAL enabled.\n")
        result = subprocess.run(["gdalinfo", "--version"], stdout=subprocess.PIPE)
        print(result.stdout.decode() + "\n")
    except Exception as e:
        print(
            "GDAL not enabled. Mosaic with GDAL requires that GDAL be installed on the cluster.\n"
        )
        print(
            "You can run setup_gdal() or setup_fuse_install() to generate the init script for install GDAL install.\n"
        )
        print(
            "After the init script is generated, you need to add the init script to your cluster and restart to complete the setup.\n"
        )
        print("Error: " + str(e))
