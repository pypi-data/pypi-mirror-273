---
name: Build quantize binary

permissions:
    contents: read

on:
    push:
        branches:
            - main
        tags:
            - "v*"
    pull_request:
        branches:
            - main
    workflow_dispatch:

jobs:
    macos-build:
        name: "Build quantize on macOS ARM64 (M1)"
        runs-on: "macos-14"
        steps:
            - uses: "actions/checkout@v4"
              with:
                  submodules: true

            - name: system info
              run: sysctl -a

            - name: make build/quantize from llama.cpp sources
              env:
                  CMAKE_ARGS: "-DLLAMA_FATAL_WARNINGS=ON -DLLAMA_METAL_EMBED_LIBRARY=ON"
              run: make quantize

            - name: file info
              run: file build/quantize-arm64-darwin

            - name: test quantize
              run: |
                  build/quantize-arm64-darwin \
                      llama.cpp/models/ggml-vocab-llama.gguf \
                      /tmp/ggml-vocab-Q4_K_M.gguf \
                      Q4_K_M

            - uses: actions/upload-artifact@v4
              with:
                  name: "quantize-arm64-darwin"
                  path: build/quantize-arm64-darwin

    linux-build:
        name: "Build quantize on Linux for ${{ matrix.arch }}"
        runs-on: "ubuntu-latest"
        strategy:
            fail-fast: true
            matrix:
                include:
                    - arch: "amd64"
                      suffix: "x86_64-linux"
                      image: quay.io/sclorg/python-312-c8s:c8s
                    - arch: "arm64"
                      suffix: "aarch64-linux"
                      image: quay.io/sclorg/python-312-c8s:c8s
        steps:
            - uses: "actions/checkout@v4"
              with:
                  submodules: true

            - name: Set up QEMU
              uses: docker/setup-qemu-action@v3

            - name: Set up Docker Buildx
              uses: docker/setup-buildx-action@v3

            - name: Pull ${{ matrix.image }} for linux/${{ matrix.arch }}
              run: |
                  docker pull --platform linux/${{ matrix.arch }} ${{ matrix.image }}

            - name: make build/quantize from llama.cpp sources
              run: |
                  set -e
                  docker run --platform linux/${{ matrix.arch }} ${{ matrix.image }} uname -a
                  docker run --platform linux/${{ matrix.arch }} \
                        -v .:/opt/app-root/src \
                        -e CMAKE_ARGS="-DLLAMA_FATAL_WARNINGS=ON" \
                        ${{ matrix.image }} \
                        make quantize

            - name: file info
              run: file build/quantize-${{ matrix.suffix }}

            - name: file symbols
              run: nm -a build/quantize-${{ matrix.suffix }} | grep -o "GLIBC.*" | sort -u

            - name: test quantize
              run: |
                  docker run --platform linux/${{ matrix.arch }} \
                      -v .:/opt/app-root/src \
                      ${{ matrix.image }} \
                      build/quantize-${{ matrix.suffix }} \
                          llama.cpp/models/ggml-vocab-llama.gguf \
                          /tmp/ggml-vocab-Q4_K_M.gguf \
                          Q4_K_M

            - uses: actions/upload-artifact@v4
              with:
                  name: "quantize-${{ matrix.suffix }}"
                  path: build/quantize-${{ matrix.suffix }}

    merge-artifacts:
        name: Merge artifacts
        runs-on: ubuntu-latest
        needs:
            - macos-build
            - linux-build
        steps:
            - name: Merge artifacts
              uses: actions/upload-artifact/merge@v4
              with:
                  name: quantize
