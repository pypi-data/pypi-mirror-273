Data Processors
===============

A data processor within the :code:`DataPipe` framework implements a modular operation on dataset features, enabling various transformations such as feature extraction, filtering, or data augmentation. Each data processor is configured by a corresponding Pydantic model, defining all the necessary parameters to customize its behavior. By encapsulating specific data processing logic, data processors provide a flexible and reusable solution for manipulating input data within the data pipeline.

The following example demonstrates how to create a data processor :code:`HuggingFaceTokenizer`:

.. code-block:: python

   from hyped.data.processors.tokenizers.hf import HuggingFaceTokenizer, HuggingFaceTokenizerConfig

   # Initialize the tokenizer configuration
   tokenizer_config = HuggingFaceTokenizerConfig(
       # Specify the tokenizer model and input text feature
       tokenizer="bert-base-uncased",
       text="text",
   )

   # Initialize the HuggingFaceTokenizer data processor
   processor = HuggingFaceTokenizer(tokenizer_config)

This example initializes a :code:`HuggingFaceTokenizer` data processor with the specified tokenizer model (:code:`bert-base-uncased`) and input text feature (:code:`text`).

For an exhaustive list of existing data processors, check the :doc:`api references <api/data.processors>`.

Data Processor Configuration
----------------------------

Every data processor is customized using a Pydantic model that specifies all the essential parameters defining its behavior. In addition to the processor-specific settings, each configuration includes two additional members:

1. **keep_input_features**: This member indicates whether the input features should be passed through the processor or not. If set to `True`, the input features will be retained in the output batch. If set to `False`, the input features will be removed from the output batch. Typically, this setting defaults to `True`.

    .. code-block:: python

       # Initialize the tokenizer configuration
       tokenizer_config = HuggingFaceTokenizerConfig(
           # Specify the tokenizer model and input text feature
           tokenizer="bert-base-uncased",
           text="text",
           # Don't pass input features through
           # Output will only contain features generated by the processor
           keep_input_features=False,
       )


2. **output_format**: This member specifies the output format of the processor in the form of a :doc:`FeatureDict <feature_access>`. It allows users to immediately specify a specific output format for the processed data. The output format can vary depending on the type of processor and the desired transformation applied to the input data.
    
    .. code-block:: python

       # Initialize the tokenizer configuration
       tokenizer_config = HuggingFaceTokenizerConfig(
           # Specify the tokenizer model and input text feature
           tokenizer="bert-base-uncased",
           text="text",
           # Specify the output format of the processor
           output_format={
               "tokenizer": {
                   "my_input_ids": "input_ids",
                   "my_attention_mask": "attention_mask"
               }
           }
       )

Custom Data Processors
----------------------

Extend the functionality of Hyped by creating custom data processors tailored to your specific requirements. Custom data processors allow you to define custom data transformations, integrate with external libraries, or apply domain-specific logic to your data pipelines.

Implementing Custom Data Processors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To implement a custom data processor in Hyped, you'll need to define a configuration class and a processor class. Here's how to get started:

1. **Define the Configuration Class**: Create a configuration class by subclassing :code:`BaseDataProcessorConfig` provided by Hyped. This class should be a Pydantic model and hold all the configurations for your processor. When defining the attributes of your configuration class, you can utilize special types to describe input features to be processed by the processor:

	- :code:`FeatureKey`: Represents a single feature key or name that identifies a specific input feature to be processed.

	- :code:`FeatureCollection`: Represents a collection of feature keys that belong to a group or category, allowing for the processing of multiple related features together.

	- :code:`FeatureDict`: Represents a dictionary of feature keys mapped to their corresponding configurations or settings, providing a structured way to define configurations for a set of features.

  For more information on this please refer to the :doc:`Feature Access Documentation <feature_access>`

2. **Define the Processor Class**: Create a processor class by subclassing :code:`BaseDataProcessor`. The processor class should be constructed from the configuration alone. You can overwrite the following functions in the processor class to customize its behavior:

    - :code:`def map_features(self, features: datasets.Features) -> datasets.Features`
      
      Takes in Hugging Face dataset features that are input to the processor and returns the features that are generated by the processor.

    - :code:`(async) def process(self, example: dict[str, Any], index: int, rank: int) -> dict[str, Any]`
      
      This function is responsible for performing the actual processing of a single example in the data pipeline. Optionally, you may define it as an async coroutine if you need asynchronous processing.

    - :code:`(async) def process(self, examples: dict[str, Any], index: int, rank: int) -> Iterator[dict[str, Any]]`
      
      This function defines a generator that yields processed examples. It allows for data augmentation or filtering by iterating through a batch of examples and yielding the processed versions. The use of the :code:`yield` keyword is required within this function for it to work as intended. Optionally, you may define it as an async coroutine for asynchronous processing.

    - :code:`def internal_batch_process(self, examples: dict[str, list[Any]], index: list[int], rank: int) -> tuple[dict[str, list[Any]], list[int]]`
      
      This function is responsible for processing a batch of examples in the form of :code:`dict[str, list[Any]]`. It should return the processed batch in the same form as well as a list of source indices indicating which input generated each output example.

Example
~~~~~~~

Below is an example of how to implement a custom data processor in Hyped:

.. code-block:: python

    from datasets import Features, Value
    from hyped.common.feature_key import FeatureKey
    from hyped.data.processors.base import BaseDataProcessorConfig, BaseDataProcessor

    # Define the Configuration Class
    class MyCustomProcessorConfig(BaseDataProcessorConfig):
        # Define configuration parameters here
        feature: FeatureKey
        parameter1: str
        parameter2: int

    # Define the Processor Class
    class MyCustomProcessor(BaseDataProcessor[BaseDataProcessorConfig]):
        def __init__(self, config: MyCustomProcessorConfig) -> None:
            super().__init__(config)

        def map_features(self, features: Features) -> Features:
            # Implement feature mapping logic here
            feature = self.config.feature.index_features(features)
            return {"out": feature}

        def process(self, example: dict[str, Any], index: int, rank: int) -> dict[str, Any]:
            # Implement custom data processing logic here
            val = self.config.feature.index_example(example)
            return {"out": val}

This example demonstrates how to define a configuration class (:code:`MyCustomProcessorConfig`) and a processor class (:code:`MyCustomProcessor`). The configuration class holds all the configurations for the processor, and the processor class is constructed from the configuration alone.

Explore further examples and use cases to learn how to leverage custom data processors effectively in your Hyped workflows.

