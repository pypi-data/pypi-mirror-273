# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import concurrent.futures
import logging
import os
import time
from typing import Optional

from rich.progress import Progress

from agenteval.defaults import MAX_NUM_THREADS
from agenteval.plan import Plan
from agenteval.runner.summary import create_markdown_summary

logger = logging.getLogger(__name__)


class Runner:
    def __init__(
        self,
        plan: Plan,
        verbose: bool,
        num_threads: Optional[int],
        work_dir: Optional[str],
    ):
        self.plan = plan
        self.work_dir = work_dir if work_dir else os.getcwd()
        self.num_tests = len(self.plan.tests)
        self.verbose = verbose
        self.num_threads = num_threads
        if not self.num_threads:
            self.num_threads = min(self.num_tests, MAX_NUM_THREADS)
        self.results = {test.name: None for test in self.plan.tests}
        self.num_failed = 0
        self.evaluator_input_token_counts = []
        self.evaluator_output_token_counts = []

    def run(self) -> int:
        self._log_run_start()

        self.start_time = time.time()
        with Progress(transient=True) as self.progress:
            self.tracker = self.progress.add_task("running...", total=self.num_tests)

            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.num_tests
            ) as executor:
                futures = [
                    executor.submit(self.run_test, test) for test in self.plan.tests
                ]
                for future in concurrent.futures.as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        raise e

        self._log_run_end()

        create_markdown_summary(
            self.work_dir, self.plan.tests, list(self.results.values()), self.verbose
        )

        return self.num_failed

    def run_test(self, test):
        target = self.plan.target_factory.create()
        evaluator = self.plan.evaluator_factory.create(
            test=test,
            target=target,
            work_dir=self.work_dir,
        )

        result = evaluator.run()
        if result.success is False:
            self.num_failed += 1

        self.progress.update(self.tracker, advance=1)
        self.results[test.name] = result
        self.evaluator_input_token_counts.append(evaluator.input_token_count)
        self.evaluator_output_token_counts.append(evaluator.output_token_count)

    def _log_run_start(self):
        logger.info(f"Starting {self.num_tests} tests with {self.num_threads} threads.")

    def _log_run_end(self):
        self._log_pass_fail_count()
        logger.info(f"Completed in {round(time.time() - self.start_time, 2)} seconds.")
        if self.verbose:
            self._log_test_result()
            self._log_evaluator_token_io()

    def _log_test_result(self):
        for _, result in self.results.items():
            logger_func = logger.info if result.success else logger.error
            logger_func(
                f"[bold {'green' if result.success else 'red'}]{result.test_name}...{'PASSED' if result.success else 'FAILED'}",
            )

    def _log_pass_fail_count(self):
        passed_count = self.num_tests - self.num_failed
        status_str = (
            f"[red]{passed_count} passed, {self.num_failed} failed."
            if self.num_failed
            else f"[green]{self.num_tests} passed."
        )
        logger_func = logger.error if self.num_failed else logger.info
        logger_func(status_str)

    def _log_evaluator_token_io(self):
        logger.info(
            f"Input tokens processed by evaluator: {sum(self.evaluator_input_token_counts)}"
        )
        logger.info(
            f"Output tokens generated by evaluator: {sum(self.evaluator_output_token_counts)}"
        )
